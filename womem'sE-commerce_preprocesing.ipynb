{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acd2e39",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb5eaa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:01.475782Z",
     "start_time": "2022-07-12T18:46:00.612313Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2901ae3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-12T18:46:50.387Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79b8b8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:02.827260Z",
     "start_time": "2022-07-12T18:46:02.546898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values: 2966\n",
      "Dataframe Dimension: 22628 Rows, 10 Columns\n"
     ]
    }
   ],
   "source": [
    "# Read and Peak at Data\n",
    "df = pd.read_csv(\"./Sentiment_analysis/Womens Clothing E-Commerce Reviews.csv\",index_col=0)\n",
    "\n",
    "# Delete missing observations for following variables\n",
    "for x in [\"Division Name\",\"Department Name\",\"Class Name\",\"Review Text\"]:\n",
    "    df = df[df[x].notnull()]\n",
    "\n",
    "# Extracting Missing Count and Unique Count by Column\n",
    "unique_count = []\n",
    "for x in df.columns:\n",
    "    unique_count.append([x,len(df[x].unique()),df[x].isnull().sum()])\n",
    "\n",
    "# Missing Values\n",
    "print(\"Missing Values: {}\".format(df.isnull().sum().sum()))\n",
    "\n",
    "# Data Dimensions\n",
    "print(\"Dataframe Dimension: {} Rows, {} Columns\".format(*df.shape))\n",
    "\n",
    "# Create New Variables: \n",
    "# Word Length\n",
    "df[\"Word Count\"] = df['Review Text'].str.split().apply(len)\n",
    "# Character Length\n",
    "df[\"Character Count\"] = df['Review Text'].apply(len)\n",
    "# Boolean for Positive and Negative Reviews\n",
    "df[\"Label\"] = 0\n",
    "df.loc[df.Rating >= 3,[\"Label\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b52e3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:10.673391Z",
     "start_time": "2022-07-12T18:46:02.828710Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenization and stemming\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_doc(doc):\n",
    "    # split the doc into word tokens\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    tokens = [w.lower() for w in tokens if not w in string.punctuation]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    # stemming\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    # filter out stop words\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens\n",
    "df['cleaned_review'] = [clean_doc(rev) for rev in df[\"Review Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9876cad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:10.687644Z",
     "start_time": "2022-07-12T18:46:10.674433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Character Count</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>[absolut, wonder, silki, sexi, comfort]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>62</td>\n",
       "      <td>303</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, sooo, happen, find, glad, bc, never, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>98</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>[high, hope, dress, realli, want, work, initi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>22</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, everi, time, wear, get, noth, great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>36</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "      <td>[shirt, veri, flatter, due, adjust, front, per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0          767   33                      NaN   \n",
       "1         1080   34                      NaN   \n",
       "2         1077   60  Some major design flaws   \n",
       "3         1049   50         My favorite buy!   \n",
       "4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                1   \n",
       "1  Love this dress!  it's sooo pretty.  i happene...       5                1   \n",
       "2  I had such high hopes for this dress and reall...       3                0   \n",
       "3  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "4  This shirt is very flattering to all due to th...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                        0       Initmates        Intimate  Intimates   \n",
       "1                        4         General         Dresses    Dresses   \n",
       "2                        0         General         Dresses    Dresses   \n",
       "3                        0  General Petite         Bottoms      Pants   \n",
       "4                        6         General            Tops    Blouses   \n",
       "\n",
       "   Word Count  Character Count  Label  \\\n",
       "0           8               53      1   \n",
       "1          62              303      1   \n",
       "2          98              500      1   \n",
       "3          22              124      1   \n",
       "4          36              192      1   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0            [absolut, wonder, silki, sexi, comfort]  \n",
       "1  [love, sooo, happen, find, glad, bc, never, wo...  \n",
       "2  [high, hope, dress, realli, want, work, initi,...  \n",
       "3        [love, everi, time, wear, get, noth, great]  \n",
       "4  [shirt, veri, flatter, due, adjust, front, per...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afacc0",
   "metadata": {},
   "source": [
    "## Count vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758e626c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:11.279022Z",
     "start_time": "2022-07-12T18:46:10.688838Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# merged words list\n",
    "X_train =[' '.join(review) for review in df_train['cleaned_review']]\n",
    "X_test=[' '.join(review) for review in df_test['cleaned_review']]\n",
    "\n",
    "# count\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_counts = count_vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cec80c",
   "metadata": {},
   "source": [
    "## n-gram vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2857b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:14.034807Z",
     "start_time": "2022-07-12T18:46:11.280106Z"
    }
   },
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "X_train_ng = ngram_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_ng = ngram_vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b0fff4",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892bb106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:17.200973Z",
     "start_time": "2022-07-12T18:46:14.035793Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "X_train_tfidf = transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab9f0d",
   "metadata": {},
   "source": [
    "## word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525e2b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.448914Z",
     "start_time": "2022-07-12T18:46:17.201905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=2964, vector_size=128, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# word2Vec\n",
    "model_w2v = Word2Vec(df[\"cleaned_review\"],vector_size=128)\n",
    "# summarize the loaded model\n",
    "print(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97430aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.594301Z",
     "start_time": "2022-07-12T18:46:18.451189Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['cleaned_review'])\n",
    "encoded_train_docs = tokenizer.texts_to_sequences(df_train['cleaned_review'])\n",
    "encoded_test_docs = tokenizer.texts_to_sequences(df_test['cleaned_review'])\n",
    "max_length = max([len(s) for s in df_train['cleaned_review']])\n",
    "X_train = pad_sequences(encoded_train_docs, \n",
    "                        maxlen=max_length, \n",
    "                        padding='post')\n",
    "y_train = pd.get_dummies(df_train['Class Name'])\n",
    "X_test = pad_sequences(encoded_test_docs, \n",
    "                        maxlen=max_length, \n",
    "                        padding='post')\n",
    "y_test = pd.get_dummies(df_test['Class Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c002408",
   "metadata": {},
   "source": [
    "# Topic Detection\n",
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9cfdca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.595189Z",
     "start_time": "2022-07-12T18:46:18.595182Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a440f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.596483Z",
     "start_time": "2022-07-12T18:46:18.596470Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate dictionary (bag of words)\n",
    "docs_dictionary = Dictionary(df['cleaned_review'])\n",
    "# transfer each document into (word_index, frequency)\n",
    "docs_corpus = [docs_dictionary.doc2bow(review) for review in df['cleaned_review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15f98a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.597346Z",
     "start_time": "2022-07-12T18:46:18.597337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.049*\"abov\" + 0.046*\"overal\" + 0.045*\"shorter\" + 0.043*\"end\" + 0.042*\"jacket\" + 0.034*\"know\" + 0.034*\"denim\" + 0.033*\"kind\" + 0.031*\"tie\" + 0.031*\"flow\" + 0.025*\"open\" + 0.022*\"heavi\" + 0.021*\"appropri\" + 0.018*\"snag\" + 0.018*\"tunic\"'),\n",
       " (1,\n",
       "  '0.079*\"skirt\" + 0.054*\"pair\" + 0.052*\"tight\" + 0.042*\"white\" + 0.039*\"side\" + 0.030*\"someth\" + 0.028*\"alway\" + 0.027*\"recommend\" + 0.025*\"weight\" + 0.021*\"slip\" + 0.020*\"fun\" + 0.019*\"tank\" + 0.019*\"thick\" + 0.016*\"lbs\" + 0.016*\"navi\"'),\n",
       " (2,\n",
       "  '0.065*\"look\" + 0.058*\"like\" + 0.041*\"would\" + 0.040*\"top\" + 0.031*\"fabric\" + 0.028*\"tri\" + 0.025*\"realli\" + 0.020*\"becaus\" + 0.019*\"much\" + 0.019*\"materi\" + 0.018*\"think\" + 0.017*\"feel\" + 0.016*\"also\" + 0.015*\"want\" + 0.015*\"cute\"'),\n",
       " (3,\n",
       "  '0.060*\"fit\" + 0.058*\"size\" + 0.055*\"veri\" + 0.047*\"wear\" + 0.031*\"order\" + 0.024*\"littl\" + 0.022*\"small\" + 0.020*\"perfect\" + 0.019*\"run\" + 0.019*\"bit\" + 0.017*\"usual\" + 0.015*\"beauti\" + 0.015*\"got\" + 0.015*\"flatter\" + 0.015*\"go\"'),\n",
       " (4,\n",
       "  '0.068*\"mayb\" + 0.057*\"figur\" + 0.056*\"blous\" + 0.033*\"ankl\" + 0.033*\"tuck\" + 0.030*\"tad\" + 0.030*\"ad\" + 0.030*\"notic\" + 0.027*\"pink\" + 0.027*\"broad\" + 0.024*\"cool\" + 0.024*\"often\" + 0.022*\"simpl\" + 0.021*\"cheap\" + 0.021*\"form\"'),\n",
       " (5,\n",
       "  '0.061*\"wash\" + 0.060*\"happi\" + 0.052*\"easi\" + 0.049*\"lace\" + 0.043*\"add\" + 0.034*\"tee\" + 0.029*\"jumpsuit\" + 0.028*\"local\" + 0.026*\"incred\" + 0.024*\"exact\" + 0.022*\"stretchi\" + 0.018*\"dri\" + 0.016*\"pencil\" + 0.015*\"bright\" + 0.014*\"rich\"'),\n",
       " (6,\n",
       "  '0.075*\"detail\" + 0.060*\"absolut\" + 0.044*\"everi\" + 0.036*\"sheer\" + 0.034*\"unfortun\" + 0.033*\"extrem\" + 0.030*\"belt\" + 0.029*\"slim\" + 0.027*\"agre\" + 0.027*\"bead\" + 0.024*\"feminin\" + 0.021*\"noth\" + 0.020*\"complaint\" + 0.020*\"help\" + 0.018*\"swing\"'),\n",
       " (7,\n",
       "  '0.139*\"dress\" + 0.033*\"get\" + 0.032*\"make\" + 0.029*\"long\" + 0.028*\"sweater\" + 0.026*\"sleev\" + 0.023*\"cut\" + 0.022*\"qualiti\" + 0.019*\"design\" + 0.018*\"right\" + 0.018*\"high\" + 0.016*\"mani\" + 0.015*\"blue\" + 0.014*\"first\" + 0.014*\"wore\"'),\n",
       " (8,\n",
       "  '0.167*\"love\" + 0.080*\"great\" + 0.058*\"color\" + 0.051*\"bought\" + 0.044*\"one\" + 0.035*\"soft\" + 0.032*\"purchas\" + 0.030*\"comfort\" + 0.026*\"super\" + 0.026*\"black\" + 0.023*\"jean\" + 0.022*\"pant\" + 0.019*\"style\" + 0.018*\"worn\" + 0.017*\"nice\"'),\n",
       " (9,\n",
       "  '0.051*\"yet\" + 0.050*\"favorit\" + 0.049*\"full\" + 0.045*\"tall\" + 0.040*\"elast\" + 0.038*\"howev\" + 0.036*\"green\" + 0.035*\"consid\" + 0.026*\"tend\" + 0.023*\"matern\" + 0.022*\"certain\" + 0.021*\"tell\" + 0.021*\"flat\" + 0.021*\"floral\" + 0.020*\"sit\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on the corpus.\n",
    "lda_model = LdaModel(corpus=docs_corpus, \n",
    "               id2word=docs_dictionary,\n",
    "               num_topics=10,\n",
    "               chunksize=100,\n",
    "               passes=10,\n",
    "               alpha='auto')\n",
    "lda_model.print_topics(num_topics=20,num_words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0a91b",
   "metadata": {},
   "source": [
    "## Probabilistic Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568fd094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.599763Z",
     "start_time": "2022-07-12T18:46:18.599753Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9347c204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.600508Z",
     "start_time": "2022-07-12T18:46:18.600501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22628"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews=[' '.join(text) for text in df['cleaned_review']]\n",
    "len(Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ada6ce5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T18:46:18.601525Z",
     "start_time": "2022-07-12T18:46:18.601517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Topic # 01 Topic # 02 Topic # 03 Topic # 04 Topic # 05 Topic # 06  \\\n",
      "0       look       love      great      dress        fit       veri   \n",
      "1       like    absolut       look       easi       true    flatter   \n",
      "2        top      color       jean    flatter       well       soft   \n",
      "3     realli    everyth      style      could     materi       well   \n",
      "4       nice    feminin    qualiti     casual      color     materi   \n",
      "5     fabric       wish       work     easili        lbs      happi   \n",
      "6      would      simpl      color       made       good       nice   \n",
      "7      color     detail     summer       slip       loos       true   \n",
      "8      shirt     materi     skinni     pretti    flatter     fabric   \n",
      "9       much       soft     bought     summer         xs     pretti   \n",
      "\n",
      "   Topic # 07 Topic # 08 Topic # 09  Topic # 10  \n",
      "0      beauti      super       size     perfect  \n",
      "1       color       cute      order        wear  \n",
      "2     sweater      comfi        run     comfort  \n",
      "3        high      shirt      small        soft  \n",
      "4      design       soft      usual      bought  \n",
      "5       skirt      littl       larg  compliment  \n",
      "6        well      short     medium        mani  \n",
      "7  embroideri    sweater     normal        jean  \n",
      "8     absolut        run      littl        pant  \n",
      "9    gorgeous     realli      would        easi  \n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
    "x_counts = vectorizer.fit_transform(Reviews)\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(x_counts)\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
    "#number of topics\n",
    "num_topics=10\n",
    "#obtain a NMF model.\n",
    "model = NMF(n_components=num_topics, init='nndsvd');\n",
    "#fit the model\n",
    "model.fit(xtfidf_norm)\n",
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict)\n",
    "\n",
    "\n",
    "print(get_nmf_topics(model, num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f027d6",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d06d6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['Review Text'])\n",
    "encoded_train_docs = tokenizer.texts_to_sequences(df_train['Review Text'])\n",
    "encoded_test_docs = tokenizer.texts_to_sequences(df_test['Review Text'])\n",
    "max_length = max([len(s) for s in df_train['Review Text']])\n",
    "X_train = pad_sequences(encoded_train_docs, \n",
    "                        maxlen=max_length, \n",
    "                        padding='post')\n",
    "y_train = pd.get_dummies(df_train['Class Name'])\n",
    "X_test = pad_sequences(encoded_test_docs, \n",
    "                        maxlen=max_length, \n",
    "                        padding='post')\n",
    "y_test = pd.get_dummies(df_test['Class Name'])\n",
    "diff_idx = y_train.columns.difference(y_test.columns)\n",
    "for idx in diff_idx:\n",
    "    y_test[idx] = [0]*len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d11bf4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embed_layer(max_length, tokenizer, wv):\n",
    "  word_index = tokenizer.word_index\n",
    "  embedding_mat = np.zeros((len(word_index)+1, 100))\n",
    "  for word, i in word_index.items():\n",
    "      try:\n",
    "          vector = wv[word]\n",
    "          embedding_mat[i] = vector\n",
    "      except:\n",
    "          continue\n",
    "  word2vec_embedding_layer = Embedding(input_dim=embedding_mat.shape[0],\n",
    "                                      output_dim=embedding_mat.shape[1], \n",
    "                                      weights=[embedding_mat],\n",
    "                                      input_length=max_length, \n",
    "                                      trainable=False)\n",
    "  return word2vec_embedding_layer\n",
    "\n",
    "# model_word2vec = Sequential()\n",
    "# model_word2vec.add(get_word2vec_embed_layer(max_length, tokenizer, model_w2v.wv))\n",
    "# model_word2vec.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "# model_word2vec.add(MaxPooling1D(pool_size=2))\n",
    "# model_word2vec.add(Flatten())\n",
    "# model_word2vec.add(Dense(20, activation='softmax'))\n",
    "# print(model_word2vec.summary())\n",
    "# model_word2vec.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model_word2vec.fit(X_train, y_train, epochs=10)\n",
    "# model_word2vec.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a12d5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/u3591577/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# merged words list\n",
    "X_train =[' '.join(review) for review in df_train['cleaned_review']]\n",
    "X_test=[' '.join(review) for review in df_test['cleaned_review']]\n",
    "\n",
    "# count\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_counts = count_vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d668661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhome/cs/u3591577/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "566/566 [==============================] - 163s 282ms/step - loss: 2.3433 - accuracy: 0.2458\n",
      "Epoch 2/10\n",
      "566/566 [==============================] - 157s 278ms/step - loss: 2.3149 - accuracy: 0.2547\n",
      "Epoch 3/10\n",
      "566/566 [==============================] - 158s 278ms/step - loss: 2.2956 - accuracy: 0.2604\n",
      "Epoch 4/10\n",
      "566/566 [==============================] - 158s 278ms/step - loss: 2.2846 - accuracy: 0.2687\n",
      "Epoch 5/10\n",
      "566/566 [==============================] - 158s 279ms/step - loss: 2.2811 - accuracy: 0.2661\n",
      "Epoch 6/10\n",
      "566/566 [==============================] - 158s 279ms/step - loss: 2.2784 - accuracy: 0.2668\n",
      "Epoch 7/10\n",
      "566/566 [==============================] - 158s 279ms/step - loss: 2.2763 - accuracy: 0.2695\n",
      "Epoch 8/10\n",
      "566/566 [==============================] - 158s 279ms/step - loss: 2.2744 - accuracy: 0.2714\n",
      "Epoch 9/10\n",
      "566/566 [==============================] - 157s 278ms/step - loss: 2.2735 - accuracy: 0.2716\n",
      "Epoch 10/10\n",
      "566/566 [==============================] - 158s 278ms/step - loss: 2.2727 - accuracy: 0.2720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15056086a7c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained bert\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text # just needed tensorflow_text\n",
    "\n",
    "bert_encoder_dir = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
    "bert_preprocess_dir = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "\n",
    "bert_preprocess_layer = hub.KerasLayer(bert_preprocess_dir)\n",
    "bert_encode_model = hub.KerasLayer(bert_encoder_dir, trainable=True)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "text_input = Input(shape=(), dtype=tf.string)\n",
    "bert_inputs = bert_preprocess_layer(text_input)\n",
    "outputs = bert_encode_model(bert_inputs)\n",
    "net = outputs['pooled_output']\n",
    "net = Dropout(0.1)(net)\n",
    "net = Dense(20, activation='softmax')(net)\n",
    "bert_model = Model(text_input, net)\n",
    "bert_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bert_model.fit(tf.constant(X_train), y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc27e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
